---
title: "CSCI454_hw3_part_4"
author: "Jen Johnson"
date: "11/13/2017"
output:
  pdf_document: default
---
# Part 4 Local PCA

## Training Data

```{r}
library(png)
filenames <- list.files(path = "/Users/jen/Dropbox/CSCI 454/hw/trainingaligned2")
```

Helper Function for converting x and y rows/colums into sector numbers 1-9.
```{r}

x.and.y.to.list.index <- function(x, y){
  if (x == 0) {
    if (y == 0) {
      final.index = 1
    } else if (y == 1) {
      final.index = 2
    } else {
      final.index = 3
    }
  } else if (x == 1){
    if (y == 0) {
      final.index = 4
    } else if (y == 1) {
      final.index = 5
    } else {
      final.index = 6
    }
  } else {
    if (y == 0) {
      final.index = 7
    } else if (y == 1) {
      final.index = 8
    } else {
      final.index = 9
    }
  }
  return(final.index)
}
```

Read in the training data and divide into sectors.
```{r}
#dim(img) 60 60
dim.img <- 60
one.third <- dim.img/3
len.array <- one.third ** 2

sectors <- array(NA, c(len.array, length(filenames), 9))

for (f in 1:length(filenames)) {
  img <- readPNG(paste("/Users/jen/Dropbox/CSCI 454/hw/trainingaligned2", filenames[f], sep = "/"))
  
  for (i in 0:2){
  min1 <- i*one.third +1
  max1 <- min1 + one.third - 1 
    
    for (j in 0:2){
      min2 <- j * one.third + 1
      max2 <- min2 + one.third - 1

      current <- img[min1:max1, min2:max2]
      to.add <- as.vector(current)
    
      # get index from x and y using helper function
      index <- x.and.y.to.list.index(i, j)
      
      # assign using indices
      sectors[,f,index] <- to.add 
    }
  }
}
```

Normalize and center each sector. Store the mean of each row in each sector to normalize the testing later. Calculate the eigen vectors and keep the top 20.
```{r}
weights <- array(NA, c(20, len.array, 9))
mean.rows.per.sector <- array(NA, c(len.array, 1 ,9))

for (i in 1:dim(sectors)[3]){
  training <- sectors[,,i]
  
  # Min-Max Scaling
  col.mins <- apply(training, 2, min)
  intermediate.result <- t(t(training)-col.mins)
  col.maxes <- apply(training, 2, max)
  denominator <- col.maxes - col.mins
  training <- t(t(intermediate.result)/denominator)
  
  # Centering
  mean.rows <- apply(training, 1, mean)
  training <- cbind(training, mean.rows)
  mean.rows.per.sector[,,i] <- mean.rows

  X <- training[ , 1:dim(training)[2]-1]-training[ ,dim(training)[2]]
  
  # Transpose
  trans.X <- t(X)

  # Covariance Matrix
  C <- X %*% trans.X
  
  # Eigen Vectors
  EV <- eigen(C)

  Top.20 <- EV$vectors[ , 1:20]

  trans.EV <- t(Top.20)
  
  weights[,,i] <- trans.EV
}
```

## Testing Data

Read in the testing data and divide into sectors.
```{r}
filenames <- list.files(path = "/Users/jen/Dropbox/CSCI 454/hw/testingaligned2")

sectors <- array(NA, c(len.array, length(filenames), 9))

for (f in 1:length(filenames)) {
  img <- readPNG(paste("/Users/jen/Dropbox/CSCI 454/hw/testingaligned2", filenames[f], sep = "/"))
  # img <- readPNG(paste("/Users/jen/Dropbox/CSCI 454/hw/alignedtesting", filenames[f], sep = "/"))
  one.third <- dim(img)[1]/3
  
  for (i in 0:2){
  min1 <- i*one.third +1
  max1 <- min1 + one.third - 1 
    
    for (j in 0:2){
      min2 <- j * one.third + 1
      max2 <- min2 + one.third - 1

      current <- img[min1:max1, min2:max2]
      to.add <- as.vector(current)
    
      # get index from x and y using helper function
      index <- x.and.y.to.list.index(i, j)
      
      # assign using indices
      sectors[,f,index] <- to.add 
    }
  }
}
```

Normalize and center each sector. Normalize using the max of the column in the testing data. Center using the mean of the row in the training data.
```{r}
for (i in 1:dim(sectors)[3]){
  testing <- sectors[,,i]

  # Min-Max Scaling
  col.mins <- apply(testing, 2, min)
  intermediate.result <- t(t(testing)-col.mins)
  col.maxes <- apply(testing, 2, max)
  denominator <- col.maxes - col.mins
  testing <- t(t(intermediate.result)/denominator)
  
  # Subtract the mean of the training data for that sector
  sectors[,,i] <- testing - mean.rows.per.sector[,,i]
}
```

Multiply testing image sectors by the weights for those sectors and store. 
```{r}
weights.x.images <- array(NA, c(20, length(filenames), 9))

for (i in 1:dim(sectors)[3]){
  current.img <- sectors[,,i]
  current.weights <- weights[,,i]
  weights.x.images[,,i] <- current.weights %*% current.img
}
```

Make a boolean where matches == TRUE and imposters == FALSE for sorting later.
```{r}
true.matches.false.imposters <- c()

for (i in 1:length(filenames)){
    j <- i+1
    while (j <= length(filenames)){
      image1 <- filenames[i]
      subject1 <- substr(image1, 2, 3)
      image2 <- filenames[j]
      subject2 <- substr(image2, 2, 3)
        
        if (subject1 == subject2) {
          true.matches.false.imposters <- c(true.matches.false.imposters, TRUE)
        } else {
          true.matches.false.imposters <- c(true.matches.false.imposters, FALSE)
        }
      j <- j+1
    }
} 
```

For each image in each sector, compare to all other images and store in a matrix. The rows of the matrix will be each comparison. There will be 9 columns in the matrix that correspond to the difference obtained from each sector.
```{r}
distance.scores.to.be.weighted <- c()

for (d in 1:dim(sectors)[3]){
  current.col.to.add <- c()
  current <- weights.x.images[,,d]
  for (i in 1:length(filenames)){
    j <- i+1
    while (j <= length(filenames)){
      weight1 <- current[, i]
      weight2 <- current[, j]
      weight.diff <- sum(abs(weight1-weight2))

      current.col.to.add <- c(current.col.to.add, weight.diff)
      j <- j+1
    }
  }
  distance.scores.to.be.weighted <- cbind(distance.scores.to.be.weighted, current.col.to.add)
  print(d)
}
```

For each sector, weight by the weight of the sector.
```{r}
weights <- c(1, 2, 1, 2, 2, 2, 1, 2, 1)

weighted.scores <- t(t(distance.scores.to.be.weighted) * weights)
```

For each comparison, sum up the weighted differences.
```{r}
total.score <- apply(weighted.scores, 1, sum)
```

Use the boolean table to sort into imposter and genuine.
```{r}
library(tidyverse)
scores <- data.frame(cbind(true.matches.false.imposters, total.score))

genuine <- scores %>% filter(true.matches.false.imposters == 1) %>% select(total.score)

imposter <- scores %>% filter(true.matches.false.imposters == 0) %>% select(total.score)
```

## Plot distribution from HW1

```{r}
library(ggplot2)
theme_update(plot.title = element_text(hjust = 0.5))

imposter <- as.data.frame(imposter)
genuine <- as.data.frame(genuine)

#Scale using the density function and plot using ggplot's geom_freqpoly.
ggplot() + geom_freqpoly(data = imposter, aes(x = total.score, y = ..density..), bins = 50, color = "red") + geom_freqpoly(data = genuine, aes(x = total.score, y = ..density..), bins = 50) + labs(title = "Distribution of Scores") + labs(x = "Match Score", y = "Scaled Frequency")
```

## DET from HW1

```{r}
FAR_vs_FRR <- NULL

#For each value of t, calculate FAR and FRR and add to dataset. 

for (t in seq(from = 100, to = 400, by = 10)){

  false_accept_count <- sum(imposter < t)
  false_accept_rate <- false_accept_count/dim(imposter)[1]
  false_reject_count <- sum(genuine > t)
  false_reject_rate <- false_reject_count/dim(genuine)[1]
  
  current_row <- c(false_accept_rate, false_reject_rate)
  FAR_vs_FRR<- rbind(FAR_vs_FRR, current_row)
}

rates_data_frame <- as.data.frame(FAR_vs_FRR)
colnames(rates_data_frame) <- c("FAR", "FRR")
ggplot(rates_data_frame, aes(x=FAR, y = FRR)) + geom_point() + geom_abline(slope = 1, intercept = 0) + labs(title = "DET curve")
```

## EER from HW1

```{r}
#Make new column containing boolean FAR > FRR. 
rates_data_frame$larger <- rates_data_frame$FAR > rates_data_frame$FRR

#Find where FAR becomes less than FRR. Use these as upper and lower boundaries to estimate the EER.
far_is_smaller <- rates_data_frame[rates_data_frame$larger=="FALSE", ]
lower_bound <- max(far_is_smaller$FAR)
far_is_larger <- rates_data_frame[rates_data_frame$larger=="TRUE", ]
upper_bound <- min(far_is_larger$FAR)

EER <- mean(lower_bound, upper_bound)
print(EER)
```


